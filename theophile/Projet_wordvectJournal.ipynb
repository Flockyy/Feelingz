{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 14:20:41.385797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 14:20:41.385822: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import texthero as hero\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_emotion.csv')\n",
    "\n",
    "dfTweet = df.drop(['tweet_id','author'],axis = 1)\n",
    "dfTweet=dfTweet.applymap(str.lower)\n",
    "\n",
    "dfJournal = pd.read_csv('Emotion_final.csv')\n",
    "dfJournal=dfJournal.applymap(str.lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14461</th>\n",
       "      <td>worry</td>\n",
       "      <td>ï¿½ miss my future, ï¿½ wanna see her  ï¿½ for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                               Text\n",
       "14461   worry  ï¿½ miss my future, ï¿½ wanna see her  ï¿½ for..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfTweet.rename(columns={'sentiment': 'Emotion', \n",
    "                           'content': 'Text'}, inplace=True)\n",
    "#dfTweet[dfTweet['index']==14461]\n",
    "dfTweet.iloc[[14461]]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       8638\n",
       "worry         8459\n",
       "happiness     5209\n",
       "sadness       5165\n",
       "love          3842\n",
       "surprise      2187\n",
       "fun           1776\n",
       "relief        1526\n",
       "hate          1323\n",
       "empty          827\n",
       "enthusiasm     759\n",
       "boredom        179\n",
       "anger          110\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTweet.Emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "happy       7029\n",
       "sadness     6265\n",
       "anger       2993\n",
       "fear        2652\n",
       "love        1641\n",
       "surprise     879\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfJournal.Emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfJournal.Emotion.replace({'happy': 'happiness'}, regex=True,inplace=True)\n",
    "# dfJournal.Emotion.replace({'enthusiasm': 'happiness'}, regex=True,inplace=True)\n",
    "# dfJournal.Emotion.replace({'fun': 'happiness'}, regex=True,inplace=True)\n",
    "# dfJournal.Emotion.replace({'relief': 'neutral'}, regex=True,inplace=True)\n",
    "# dfJournal.Emotion.replace({'empty': 'neutral'}, regex=True,inplace=True)\n",
    "# dfJournal.Emotion.replace({'boredom': 'neutral'}, regex=True,inplace=True)\n",
    "# dfJournal.Emotion.replace({'hate': 'sadness'}, regex=True,inplace=True)\n",
    "# dfJournal.Emotion.replace({'worry': 'surprise'}, regex=True,inplace=True)\n",
    "# dfJournal.Emotion.replace({'fear': 'anger'}, regex=True,inplace=True)\n",
    "\n",
    "# dfJournal = dfJournal.drop(dfJournal[dfJournal.Emotion == 'surprise'].index)\n",
    "dfJournal = dfJournal.drop(dfJournal[dfJournal.Emotion == 'surprise'].index)\n",
    "# dfJournal\n",
    "\n",
    "dfJournal.Emotion.replace({'happy': 0}, regex=True,inplace=True)\n",
    "dfJournal.Emotion.replace({'sadness': 1}, regex=True,inplace=True)\n",
    "dfJournal.Emotion.replace({'anger': 2}, regex=True,inplace=True)\n",
    "dfJournal.Emotion.replace({'fear': 3}, regex=True,inplace=True)\n",
    "dfJournal.Emotion.replace({'love': 4}, regex=True,inplace=True)\n",
    "nb_labels = len(dfJournal.Emotion.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop = list(set(stop) - set(['not','no']))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def tweets_cleaning(tweet, remove_emojis=True):\n",
    "    \"\"\"Apply function to a clean a tweet\"\"\"\n",
    "    tweet = tweet.lower().strip()\n",
    "    # romove urls\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    tweet = url.sub(r'',tweet)\n",
    "    # remove html tags\n",
    "    html = re.compile(r'<.*?>')\n",
    "    tweet = html.sub(r'',tweet)\n",
    "    # remove punctuation\n",
    "    operator = str.maketrans('','',string.punctuation) #????\n",
    "    tweet = tweet.translate(operator)\n",
    "    if remove_emojis:\n",
    "        tweet = tweet.encode('ascii', 'ignore').decode('utf8').strip()\n",
    "    lambda tweet: re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "    lambda tweet: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', tweet)\n",
    "    tweet=re.sub(r'@[A-Za-z0-9]+',\" \",tweet) ##Removing the usernames\n",
    "    tweet=re.sub(r'^[A-Za-z0-9.!?]+',\" \",tweet) ##Removing digits and punctuations\n",
    "    tweet=re.sub(r'https?://[A-Za-z0-9./]+',\" \",tweet) ## removing links\n",
    "    tweet=re.sub(r' +',\" \",tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"\\'s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \" have \", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot \", tweet)\n",
    "    tweet = re.sub(r\"n't\", \" not \", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" would \", tweet)\n",
    "    tweet = re.sub(r\"\\'ll\", \" will \", tweet)\n",
    "    tweet = re.sub(r\"\\'scuse\", \" excuse \", tweet)\n",
    "    tweet = tweet.strip(' ')\n",
    "    tweet = tweet.strip('. .')\n",
    "    tweet = tweet.replace('.',' ')\n",
    "    tweet = tweet.replace('-',' ')\n",
    "    tweet = tweet.replace(\"’\", \"'\").replace(\"′\", \"'\").replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\n",
    "    tweet = tweet.replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\n",
    "    tweet = tweet.replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    tweet = tweet.replace(\"don't\", \"do not\").replace(\"didn't\", \"did not\").replace(\"im\",\"i am\").replace(\"it's\", \"it is\")\n",
    "    tweet = tweet.replace(\",000,000\", \"m\").replace(\"n't\", \" not\").replace(\"what's\", \"what is\")\n",
    "    tweet = tweet.replace(\",000\", \"k\").replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\n",
    "    tweet = tweet.replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\n",
    "    tweet = tweet.replace(\"didnt\",\"did not\")\n",
    "    tweet = re.sub('\\s+', ' ', tweet)\n",
    "    tweet=tweet.split()\n",
    "    tweet=[lemmatizer.lemmatize(word) for word in tweet if word not in stop]\n",
    "    tweet=' '.join(word for word in tweet)\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>1</td>\n",
       "      <td>not feel humiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>1</td>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>2</td>\n",
       "      <td>grabbing minute post feel greedy wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>4</td>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>2</td>\n",
       "      <td>feeling grouchy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21454</th>\n",
       "      <td>melissa stared at her friend in dism</td>\n",
       "      <td>3</td>\n",
       "      <td>stared friend dism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21455</th>\n",
       "      <td>successive state elections have seen the gover...</td>\n",
       "      <td>3</td>\n",
       "      <td>state election seen governing party pummelled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21456</th>\n",
       "      <td>vincent was irritated but not dismay</td>\n",
       "      <td>3</td>\n",
       "      <td>irritated not dismay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21457</th>\n",
       "      <td>kendall-hume turned back to face the dismayed ...</td>\n",
       "      <td>3</td>\n",
       "      <td>turned back face dismayed coup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21458</th>\n",
       "      <td>i am dismayed , but not surpris</td>\n",
       "      <td>3</td>\n",
       "      <td>dismayed not surpris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20580 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Emotion  \\\n",
       "0                                i didnt feel humiliated        1   \n",
       "1      i can go from feeling so hopeless to so damned...        1   \n",
       "2       im grabbing a minute to post i feel greedy wrong        2   \n",
       "3      i am ever feeling nostalgic about the fireplac...        4   \n",
       "4                                   i am feeling grouchy        2   \n",
       "...                                                  ...      ...   \n",
       "21454               melissa stared at her friend in dism        3   \n",
       "21455  successive state elections have seen the gover...        3   \n",
       "21456               vincent was irritated but not dismay        3   \n",
       "21457  kendall-hume turned back to face the dismayed ...        3   \n",
       "21458                    i am dismayed , but not surpris        3   \n",
       "\n",
       "                                              clean_text  \n",
       "0                                    not feel humiliated  \n",
       "1      go feeling hopeless damned hopeful around some...  \n",
       "2                 grabbing minute post feel greedy wrong  \n",
       "3      ever feeling nostalgic fireplace know still pr...  \n",
       "4                                        feeling grouchy  \n",
       "...                                                  ...  \n",
       "21454                                 stared friend dism  \n",
       "21455  state election seen governing party pummelled ...  \n",
       "21456                               irritated not dismay  \n",
       "21457                     turned back face dismayed coup  \n",
       "21458                               dismayed not surpris  \n",
       "\n",
       "[20580 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfJournal\n",
    "df['clean_text'] = df['Text'].apply(tweets_cleaning)\n",
    "\n",
    "train_data=df.iloc[:,-1].values\n",
    "df['clean_text']=train_data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.random.randint(0,len(df),1200)\n",
    "Y_array = np.array(df[\"Emotion\"])\n",
    "X_array = np.array(df['clean_text'])\n",
    "\n",
    "\n",
    "X_test=X_array[idx[0:1000]]\n",
    "y_test=Y_array[idx[0:1000]]\n",
    "X_virgin = X_array[idx[1001:1200]]\n",
    "y_virgin = Y_array[idx[1001:1200]]\n",
    "X_train=np.delete(X_array,idx,axis=0)\n",
    "y_train=np.delete(Y_array,idx,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 4, 1, 0, 0, 0, 1, 0, 4, 0, 0, 0,\n",
       "       4, 1, 1, 0, 0, 0, 0, 2, 1, 0, 1, 2, 0, 3, 0, 4, 2, 0, 1, 1, 3, 1,\n",
       "       1, 1, 3, 1, 0, 2, 0, 4, 2, 0, 3, 1, 1, 2, 1, 1, 2, 0, 1, 2, 3, 0,\n",
       "       1, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 1, 0, 3, 0, 0, 0, 1, 4, 0, 2, 1,\n",
       "       1, 3, 1, 1, 2, 1, 2, 3, 0, 4, 2, 0, 3, 2, 1, 0, 0, 2, 2, 1, 1, 1,\n",
       "       4, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 1, 1, 1, 0, 2, 0, 2, 0,\n",
       "       0, 3, 3, 3, 2, 1, 0, 1, 3, 3, 4, 1, 0, 0, 0, 2, 0, 1, 1, 0, 0, 2,\n",
       "       1, 0, 4, 0, 3, 4, 1, 2, 4, 2, 1, 1, 4, 1, 1, 0, 0, 0, 0, 1, 4, 0,\n",
       "       0, 0, 0, 2, 1, 0, 1, 0, 1, 0, 4, 0, 1, 3, 1, 0, 2, 2, 2, 2, 0, 0,\n",
       "       4, 4, 1, 4, 2, 1, 0, 0, 3, 1, 3, 2, 3, 3, 3, 0, 0, 1, 2, 1, 1, 1,\n",
       "       0, 4, 3, 1, 4, 0, 2, 1, 1, 2, 0, 0, 0, 1, 1, 0, 1, 0, 3, 1, 1, 4,\n",
       "       2, 3, 2, 0, 4, 0, 0, 0, 3, 0, 0, 2, 1, 1, 3, 0, 3, 0, 2, 0, 4, 0,\n",
       "       1, 1, 3, 0, 1, 4, 1, 1, 2, 1, 1, 4, 0, 0, 0, 1, 1, 2, 2, 1, 3, 3,\n",
       "       0, 4, 1, 3, 1, 3, 2, 1, 1, 3, 1, 2, 0, 4, 0, 1, 0, 4, 0, 2, 3, 1,\n",
       "       1, 0, 4, 0, 2, 0, 1, 1, 1, 3, 3, 1, 3, 0, 1, 0, 1, 4, 3, 2, 1, 0,\n",
       "       1, 3, 2, 1, 1, 3, 1, 3, 0, 1, 0, 1, 1, 2, 3, 1, 3, 0, 0, 0, 0, 1,\n",
       "       0, 1, 3, 1, 4, 1, 1, 0, 1, 1, 3, 0, 2, 1, 1, 3, 0, 3, 1, 1, 4, 2,\n",
       "       4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 3, 0, 4, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 2, 3, 0, 2, 0, 2, 0, 0, 1, 0, 2, 4, 0, 3, 0, 1, 1, 0, 2, 1, 0,\n",
       "       3, 0, 0, 1, 0, 3, 1, 1, 4, 2, 2, 1, 0, 1, 3, 4, 0, 1, 2, 1, 2, 0,\n",
       "       2, 4, 1, 0, 0, 1, 2, 0, 1, 0, 0, 2, 2, 1, 2, 2, 4, 4, 0, 4, 1, 0,\n",
       "       1, 0, 1, 2, 0, 3, 1, 2, 0, 0, 1, 0, 1, 3, 3, 2, 4, 1, 2, 0, 1, 0,\n",
       "       1, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       2, 0, 3, 2, 1, 0, 2, 1, 2, 1, 1, 0, 1, 0, 3, 1, 2, 1, 3, 0, 0, 0,\n",
       "       3, 0, 2, 1, 2, 2, 0, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 1, 2, 0, 1, 1,\n",
       "       3, 0, 1, 2, 0, 4, 3, 0, 3, 0, 0, 2, 0, 4, 3, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 2, 1, 1, 0, 3, 0, 1, 1, 0, 1, 3, 3, 0, 1, 4, 3, 3, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 4, 0, 0, 2, 1, 0, 1, 1, 3, 1, 0, 4, 2, 3, 3, 2, 1,\n",
       "       3, 0, 3, 1, 1, 3, 2, 4, 3, 0, 1, 1, 3, 0, 0, 4, 4, 0, 1, 1, 2, 3,\n",
       "       0, 1, 2, 1, 0, 0, 2, 4, 0, 2, 2, 0, 0, 4, 1, 0, 0, 0, 0, 4, 1, 1,\n",
       "       4, 3, 0, 0, 0, 1, 1, 0, 1, 2, 2, 2, 0, 1, 1, 1, 0, 0, 2, 3, 3, 0,\n",
       "       0, 3, 3, 0, 1, 2, 0, 2, 3, 2, 0, 0, 0, 0, 0, 1, 1, 1, 4, 0, 1, 0,\n",
       "       4, 1, 2, 1, 4, 0, 0, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0,\n",
       "       2, 0, 0, 3, 3, 3, 3, 0, 0, 0, 3, 3, 1, 0, 1, 2, 1, 2, 0, 2, 0, 4,\n",
       "       1, 1, 1, 1, 3, 3, 1, 1, 0, 0, 2, 0, 1, 2, 1, 0, 0, 3, 0, 1, 2, 4,\n",
       "       0, 3, 0, 3, 3, 1, 1, 2, 1, 2, 1, 3, 0, 0, 2, 2, 1, 1, 0, 4, 0, 1,\n",
       "       0, 0, 0, 1, 3, 0, 4, 0, 0, 1, 1, 0, 3, 0, 0, 3, 2, 2, 0, 3, 0, 0,\n",
       "       4, 3, 1, 3, 3, 0, 1, 4, 3, 4, 1, 0, 1, 0, 3, 0, 1, 0, 2, 1, 0, 2,\n",
       "       1, 0, 3, 4, 1, 0, 0, 1, 1, 3, 0, 4, 3, 1, 0, 3, 2, 1, 3, 0, 0, 1,\n",
       "       1, 0, 3, 1, 0, 0, 3, 3, 4, 0, 1, 2, 0, 2, 0, 0, 1, 0, 3, 4, 0, 2,\n",
       "       4, 0, 1, 1, 1, 1, 0, 4, 2, 3, 4, 2, 0, 2, 2, 0, 4, 0, 0, 0, 1, 3,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 3, 4, 4, 2, 0, 1, 1,\n",
       "       0, 2, 0, 0, 0, 3, 0, 0, 0, 0, 2, 2, 1, 4, 0, 3, 2, 4, 1, 4, 1, 1,\n",
       "       1, 1, 3, 1, 3, 3, 1, 0, 0, 4, 0, 4, 3, 0, 0, 0, 3, 0, 1, 1, 0, 4,\n",
       "       0, 0, 1, 3, 3, 2, 3, 1, 1, 0, 0, 0, 2, 0, 2, 2, 0, 0, 1, 0, 2, 0,\n",
       "       1, 0, 0, 1, 2, 1, 0, 4, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 14:20:48.687158: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-05-13 14:20:48.687205: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PC-TheophileZ): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 14:20:48.687512: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "vectorizer = TextVectorization(max_tokens=40000,output_mode='int', output_sequence_length=35,standardize='lower_and_strip_punctuation')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not feel humiliated',\n",
       "       'go feeling hopeless damned hopeful around someone care awake',\n",
       "       'grabbing minute post feel greedy wrong', ...,\n",
       "       'irritated not dismay', 'turned back face dismayed coup',\n",
       "       'dismayed not surpris'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.adapt(X_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16040"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = \"glove.twitter.27B.100d.txt\"\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 14486 words (1554 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not feel humiliated',\n",
       "       'go feeling hopeless damned hopeful around someone care awake',\n",
       "       'grabbing minute post feel greedy wrong', ...,\n",
       "       'irritated not dismay', 'turned back face dismayed coup',\n",
       "       'dismayed not surpris'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 35)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 35, 100)           1604200   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 35, 100)           0         \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 35, 128)           12928     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 7, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 7, 128)            16512     \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 1, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 1, 128)            16512     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,667,309\n",
      "Trainable params: 63,109\n",
      "Non-trainable params: 1,604,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    #embeddings_initializer = \"uniform\",\n",
    "    trainable=False,\n",
    ")\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, restore_best_weights=True)\n",
    "\n",
    "#int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "input = tf.keras.Input(shape=(1,), dtype='string')\n",
    "\n",
    "x = vectorizer(input)\n",
    "#x = layers.Dropout(0.2)(x)\n",
    "x = embedding_layer(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(128, 1, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 1, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 1, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "preds = layers.Dense(nb_labels, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 35)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 35, 100)           1604200   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 35, 100)           0         \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 35, 200)          160800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 200)              240800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,026,405\n",
      "Trainable params: 422,205\n",
      "Non-trainable params: 1,604,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = tf.keras.Input(shape=(1,), dtype='string')\n",
    "\n",
    "x = vectorizer(input)\n",
    "#x = layers.Dropout(0.2)(x)\n",
    "x = embedding_layer(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Bidirectional(tf.keras.layers.LSTM(100,return_sequences=True))(x)\n",
    "x = layers.Bidirectional(tf.keras.layers.LSTM(100))(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "preds = layers.Dense(nb_labels, activation=\"softmax\")(x)\n",
    "modelLSTM = tf.keras.Model(input, preds)\n",
    "modelLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "class DCNN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 emb_dim=128,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=256,\n",
    "                 nb_classes=4,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name='dcnn'):\n",
    "        super(DCNN,self).__init__(name=name)\n",
    "\n",
    "        self.embeddings=layers.Embedding(vocab_size,emb_dim,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "\n",
    "        self.bigram=layers.Conv1D(filters=nb_filters,kernel_size=2,\n",
    "                                  padding='valid',activation='relu')\n",
    "\n",
    "        self.trigram=layers.Conv1D(filters=nb_filters,kernel_size=3,\n",
    "                                  padding='valid',activation='relu')\n",
    "\n",
    "        self.fourgram=layers.Conv1D(filters=nb_filters,kernel_size=4,\n",
    "                                  padding='valid',activation='relu')   \n",
    "\n",
    "        self.pooling=layers.GlobalMaxPool1D()\n",
    "\n",
    "        self.dense_1=layers.Dense(units=FFN_units,activation='relu')\n",
    "        self.dropout=layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "\n",
    "        self.dense_2=layers.Dense(units=nb_classes,activation='softmax')  \n",
    "\n",
    "    def call(self,inputs,training):\n",
    "        x=self.embeddings(inputs)\n",
    "        x_1=self.bigram(x)\n",
    "        x_1=self.pooling(x_1)\n",
    "        #x_2=self.trigram(x)\n",
    "        #x_2=self.pooling(x_2)\n",
    "        #x_3=self.bigram(x)\n",
    "        #x_3=self.pooling(x_3)\n",
    "\n",
    "        merged=tf.concat([x_1],axis=-1)\n",
    "        merged=self.dense_1(merged)\n",
    "        merged=self.dropout(merged,training)\n",
    "        output=self.dense_2(merged)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
    "# x_val = vectorizer(np.array([[s] for s in X_test])).numpy()\n",
    "# x_virgin = vectorizer(np.array([[s] for s in X_virgin])).numpy()\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_test)\n",
    "y_virgin = np.array(y_virgin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "152/152 [==============================] - 34s 172ms/step - loss: 1.2303 - acc: 0.5154 - val_loss: 0.9912 - val_acc: 0.6130\n",
      "Epoch 2/100\n",
      "152/152 [==============================] - 25s 166ms/step - loss: 0.9516 - acc: 0.6371 - val_loss: 0.8019 - val_acc: 0.6880\n",
      "Epoch 3/100\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.8169 - acc: 0.6918 - val_loss: 0.6551 - val_acc: 0.7670\n",
      "Epoch 4/100\n",
      "152/152 [==============================] - 27s 175ms/step - loss: 0.6969 - acc: 0.7379 - val_loss: 0.5570 - val_acc: 0.8050\n",
      "Epoch 5/100\n",
      "152/152 [==============================] - 27s 174ms/step - loss: 0.5891 - acc: 0.7812 - val_loss: 0.4895 - val_acc: 0.8160\n",
      "Epoch 6/100\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.5095 - acc: 0.8107 - val_loss: 0.3938 - val_acc: 0.8520\n",
      "Epoch 7/100\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.4486 - acc: 0.8343 - val_loss: 0.3555 - val_acc: 0.8630\n",
      "Epoch 8/100\n",
      "152/152 [==============================] - 27s 176ms/step - loss: 0.3942 - acc: 0.8545 - val_loss: 0.3241 - val_acc: 0.8710\n",
      "Epoch 9/100\n",
      "152/152 [==============================] - 27s 176ms/step - loss: 0.3659 - acc: 0.8633 - val_loss: 0.3100 - val_acc: 0.8820\n",
      "Epoch 10/100\n",
      "152/152 [==============================] - 27s 176ms/step - loss: 0.3264 - acc: 0.8765 - val_loss: 0.3067 - val_acc: 0.8800\n",
      "Epoch 11/100\n",
      "152/152 [==============================] - 27s 176ms/step - loss: 0.3012 - acc: 0.8883 - val_loss: 0.2751 - val_acc: 0.8940\n",
      "Epoch 12/100\n",
      "152/152 [==============================] - 27s 176ms/step - loss: 0.2814 - acc: 0.8963 - val_loss: 0.2611 - val_acc: 0.8970\n",
      "Epoch 13/100\n",
      "152/152 [==============================] - 27s 178ms/step - loss: 0.2559 - acc: 0.9037 - val_loss: 0.2464 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "152/152 [==============================] - 27s 177ms/step - loss: 0.2436 - acc: 0.9070 - val_loss: 0.2327 - val_acc: 0.9080\n",
      "Epoch 15/100\n",
      "152/152 [==============================] - 27s 175ms/step - loss: 0.2211 - acc: 0.9143 - val_loss: 0.2293 - val_acc: 0.9080\n",
      "Epoch 16/100\n",
      "152/152 [==============================] - 27s 178ms/step - loss: 0.2203 - acc: 0.9151 - val_loss: 0.2301 - val_acc: 0.9060\n",
      "Epoch 17/100\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2084 - acc: 0.9183 - val_loss: 0.2376 - val_acc: 0.9000\n",
      "Epoch 18/100\n",
      "152/152 [==============================] - 23s 153ms/step - loss: 0.1953 - acc: 0.9249 - val_loss: 0.2473 - val_acc: 0.9020\n",
      "Epoch 19/100\n",
      "152/152 [==============================] - 27s 179ms/step - loss: 0.1871 - acc: 0.9260 - val_loss: 0.2271 - val_acc: 0.9020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90f56ffc10>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLSTM.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.0005), metrics=[\"acc\"]\n",
    ")\n",
    "modelLSTM.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_test, y_val),callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 0.3173 - acc: 0.8790 - val_loss: 0.2785 - val_acc: 0.8970\n",
      "Epoch 2/100\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.3016 - acc: 0.8846 - val_loss: 0.2951 - val_acc: 0.8890\n",
      "Epoch 3/100\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2898 - acc: 0.8893 - val_loss: 0.2747 - val_acc: 0.8950\n",
      "Epoch 4/100\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2902 - acc: 0.8870 - val_loss: 0.2608 - val_acc: 0.8900\n",
      "Epoch 5/100\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2838 - acc: 0.8893 - val_loss: 0.2551 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2705 - acc: 0.8948 - val_loss: 0.2544 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2606 - acc: 0.8962 - val_loss: 0.2830 - val_acc: 0.8950\n",
      "Epoch 8/100\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2562 - acc: 0.9002 - val_loss: 0.2526 - val_acc: 0.8900\n",
      "Epoch 9/100\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2624 - acc: 0.8952 - val_loss: 0.2619 - val_acc: 0.8980\n",
      "Epoch 10/100\n",
      "152/152 [==============================] - 2s 15ms/step - loss: 0.2462 - acc: 0.9051 - val_loss: 0.2556 - val_acc: 0.8980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90f9836bb0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.0009), metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_test, y_val),callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.92      0.93      0.93        74\n",
      "     sadness       1.00      0.94      0.97        69\n",
      "       anger       0.84      1.00      0.91        16\n",
      "        fear       1.00      0.97      0.98        31\n",
      "        love       0.70      0.78      0.74         9\n",
      "\n",
      "    accuracy                           0.94       199\n",
      "   macro avg       0.89      0.92      0.91       199\n",
      "weighted avg       0.94      0.94      0.94       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_Y = modelLSTM.predict(X_virgin)\n",
    "y_classes = predicted_Y.argmax(axis=-1)\n",
    "y_classes\n",
    "from sklearn import metrics\n",
    "categories = ['happy','sadness','anger','fear','love']\n",
    "print(metrics.classification_report(y_virgin, y_classes, target_names=categories))\n",
    "test = ['I feel a bit sad today','Today, mom died','Jane and I argued yesterday, despite that ','I am not happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_KaggleLSTM/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_KaggleLSTM/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f90f817fc40> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f90fac40c10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f90f8b75eb0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f90f418c2e0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "#modelLSTM.save('model_KaggleLSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import models\n",
    "# model_2 = models.load_model('model_KaggleLSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.96      0.88      0.92        74\n",
      "     sadness       0.96      0.96      0.96        69\n",
      "       anger       0.84      1.00      0.91        16\n",
      "        fear       0.97      0.97      0.97        31\n",
      "        love       0.67      0.89      0.76         9\n",
      "\n",
      "    accuracy                           0.93       199\n",
      "   macro avg       0.88      0.94      0.90       199\n",
      "weighted avg       0.94      0.93      0.93       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_Y = model.predict(X_virgin)\n",
    "y_classes = predicted_Y.argmax(axis=-1)\n",
    "y_classes\n",
    "from sklearn import metrics\n",
    "categories = ['happy','sadness','anger','fear','love']\n",
    "print(metrics.classification_report(y_virgin, y_classes, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.97      0.91      0.94        74\n",
      "     sadness       0.99      0.96      0.97        69\n",
      "       anger       0.80      1.00      0.89        16\n",
      "        fear       0.97      0.97      0.97        31\n",
      "        love       0.75      1.00      0.86         9\n",
      "\n",
      "    accuracy                           0.94       199\n",
      "   macro avg       0.89      0.97      0.92       199\n",
      "weighted avg       0.95      0.94      0.95       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "categories = ['happy','sadness','anger','fear','love']\n",
    "\n",
    "print(metrics.classification_report(y_virgin, y_classes, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5828/841791412.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'I feel a bit sad today'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Today, mom died'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Jane and I argued yesterday, despite that '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'I am not happy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# from sklearn import metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test = ['I feel a bit sad today','Today, mom died','Jane and I argued yesterday, despite that ','I am not happy']\n",
    "predicted_Y = model.predict(test)\n",
    "y_classes = predicted_Y.argmax(axis=-1)\n",
    "y_classes\n",
    "# from sklearn import metrics\n",
    "# categories = ['happy','sadness','anger','fear','love']\n",
    "# print(metrics.classification_report(y_virgin, y_classes, target_names=categories))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e47ff65e698837991555106773f24a709ab70f1ee4f12e4b360efeec51aba84"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
